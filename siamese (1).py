# -*- coding: utf-8 -*-
"""Siamese.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EUq7ykSe_8kFTciWKj4PZ3HZNCTpMlDk
"""

from google.colab import drive
drive.mount('/content/drive') 
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import tensorflow as tf
df = pd.read_csv("/content/drive/MyDrive/sign_data/train_data.csv")
df.head()
path = '/content/drive/MyDrive/sign_data/train/'
testpath='/content/drive/MyDrive/sign_data/test/'
df['1'][3]
test=pd.read_csv("/content/drive/MyDrive/sign_data/test_data.csv")
test.head()
test['1'][3]

from os import name
from google.colab.patches import cv2_imshow
x = []
y = []
for ind in tqdm(df.index):
    name1 = df['068/09_068.png'][ind]
    name2 = df['068_forg/03_0113068.PNG'][ind]
    img1 = cv2.cvtColor(cv2.imread(path+str(name1)), cv2.COLOR_BGR2GRAY)
    img2 = cv2.cvtColor(cv2.imread(path+str(name2)), cv2.COLOR_BGR2GRAY)
    img1 = np.array(img1).astype('float32')/255
    img2 = np.array(img2).astype('float32')/255
    img1 = cv2.resize(img1, (128,128), cv2.INTER_CUBIC)
    img2 = cv2.resize(img2, (128,128), cv2.INTER_CUBIC)
    x += [[img1, img2]]
    y += [df['1'][ind]]

x = np.array(x)
y = np.array(y)
x_train1 = x[:23205,0]
x_train2 = x[:23205,1]
from google.colab.patches import cv2_imshow
from tensorflow.keras import Input
from tensorflow.keras import layers
from tensorflow.keras import regularizers
def euclidean_distance(vects):
    x, y = vects
    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)
    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))

from tensorflow.keras import layers, regularizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator

input = layers.Input((128, 128, 1))
x = tf.keras.layers.BatchNormalization()(input)
x = layers.Conv2D(32, (5, 5), activation="relu",kernel_regularizer=regularizers.L2(l2=2e-4),bias_regularizer=regularizers.L2(2e-4))(x)
x = layers.Dropout(0.3)(x)  # Increase dropout
x = layers.AveragePooling2D(pool_size=(2, 2))(x)
x = layers.Dropout(0.3)(x)
x = layers.Conv2D(32, (5, 5),kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),   bias_regularizer=regularizers.L2(2e-4), activation="relu")(x)
x = layers.Dropout(0.4)(x)
x = layers.AveragePooling2D(pool_size=(2, 2))(x)
x = layers.Dropout(0.3)(x)
x = layers.Flatten()(x)

x = tf.keras.layers.BatchNormalization()(x)
x = layers.Dense(128, activation="relu")(x)
embedding_network = tf.keras.Model(input, x)

early_stopping = EarlyStopping(patience=3, restore_best_weights=True)
input_1 = layers.Input((128, 128, 1))
input_2 = layers.Input((128, 128, 1))

# As mentioned above, Siamese Network share weights between
# tower networks (sister networks). To allow this, we will use
# same embedding network for both tower networks.
tower_1 = embedding_network(input_1)
tower_2 = embedding_network(input_2)
opt = tf.keras.optimizers.Adam(learning_rate=0.0001)
merge_layer = layers.Lambda(euclidean_distance)([tower_1, tower_2])
normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)
output_layer = layers.Dense(1, activation="sigmoid")(normal_layer)
siamese = tf.keras.Model(inputs=[input_1, input_2], outputs=output_layer)
siamese.compile(loss='binary_crossentropy', optimizer=opt, metrics=["accuracy"])
siamese.summary()

siamese.fit(
    [x_train1,x_train2],
    y,
    validation_split = 0.1,
    batch_size = 16,
    epochs = 10,
    verbose = 1
    
)

siamese.save('/content/drive/MyDrive/Models/siamese11.h5')

new_model=tf.keras.models.load_model('/content/drive/MyDrive/Models/siamese11.h5')
new_model.summary()

# Load the test dataset
test = pd.read_csv("/content/drive/MyDrive/sign_data/test_data.csv")
path = '/content/drive/MyDrive/sign_data/test/'

# Preprocess test images
x_test = []
y_test = []
for ind in tqdm(test.index):
    name1test = test['068/09_068.png'][ind]
    name2test = test['068_forg/03_0113068.PNG'][ind]
    img1 = cv2.cvtColor(cv2.imread(path + str(name1test)), cv2.COLOR_BGR2GRAY)
    img2 = cv2.cvtColor(cv2.imread(path + str(name2test)), cv2.COLOR_BGR2GRAY)
    img1 = np.array(img1).astype('float32') / 255
    img2 = np.array(img2).astype('float32') / 255
    img1 = cv2.resize(img1, (128, 128), cv2.INTER_CUBIC)
    img2 = cv2.resize(img2, (128, 128), cv2.INTER_CUBIC)
    x_test += [[img1, img2]]
    y_test += [test['1'][ind]]

import seaborn as sns
from sklearn.metrics import confusion_matrix
# Make predictions on the test set
x_test0 = np.array([item[0] for item in x_test])
x_test1 = np.array([item[1] for item in x_test])

y_pred = siamese.predict([x_test0, x_test1])

# Convert probabilities to class labels
#y_pred_classes = np.argmax(y_pred, axis=1)
y_pred_classes = (y_pred > 0.5).astype(int)

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)

# Plot the confusion matrix
#labels = ['Real', 'Forged']
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

TN=2736
FP=36
FN=255
TP=2720
Accuracy= (TP+TN)/(TP+TN+FP+FN)*100
Precision=TP/(TP+FP)*100
Recall = TP / (TP + FN)*100
F1_Score = 2*(Recall * Precision) / (Recall + Precision)
Accuracy,Precision,Recall,F1_Score